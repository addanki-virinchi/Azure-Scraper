# Standalone Phase 2 Processor - Complete Guide

## Overview
The `standalone_phase2_processor.py` script is a completely independent Phase 2 processor that can process Phase 1 CSV files without requiring Phase 1 scraping. It extracts detailed school information by visiting "know_more_link" URLs and consolidates the data with original Phase 1 information.

## Key Features

### ‚úÖ **Independent Operation**
- **No Phase 1 dependency** - Works directly with existing Phase 1 CSV files
- **Standalone execution** - Complete self-contained processing
- **Flexible input** - Accepts any Phase 1 CSV file with know_more_link URLs

### ‚úÖ **Robust Data Processing**
- **Incremental CSV writing** - Each record saved immediately for crash protection
- **Crash recovery** - Can resume processing by skipping already processed records
- **Comprehensive data extraction** - Extracts 20+ fields from school detail pages
- **Data validation** - Tracks extraction success and field completeness

### ‚úÖ **Production Ready**
- **Error handling** - Comprehensive exception handling and retry mechanisms
- **Progress tracking** - Real-time progress updates and time estimates
- **Logging** - Detailed logging for debugging and monitoring
- **Performance optimized** - Fast processing with browser optimizations

## Configuration

### **Input File Setup**
Edit the configuration section in `standalone_phase2_processor.py`:

```python
# ===== CONFIGURATION SECTION =====
# Specify the Phase 1 CSV file to process
INPUT_CSV_FILE = "GOA_phase1_complete_20250809_103414.csv"  # Change this to your Phase 1 CSV file

# Optional: Limit processing for testing (set to None for all records)
MAX_RECORDS_TO_PROCESS = None  # Set to a number like 10 for testing, None for all
```

### **Testing Mode**
For testing, set `MAX_RECORDS_TO_PROCESS = 10` to process only the first 10 records.

## Input Requirements

### **Required CSV Structure**
The input Phase 1 CSV file must contain:
- `know_more_link` column with valid URLs
- `school_name` column for identification
- `udise_code` column for crash recovery (optional but recommended)

### **Example Input File**
```csv
school_name,know_more_link,udise_code,state,district
SCHOOL A,https://kys.udiseplus.gov.in/#/schooldetail/1234/12,1234,GOA,NORTH GOA
SCHOOL B,https://kys.udiseplus.gov.in/#/schooldetail/5678/12,5678,GOA,NORTH GOA
```

## Output Structure

### **Output File Naming**
`{STATE_NAME}_phase2_complete_{timestamp}.csv`

Example: `GOA_phase2_complete_20250810_150657.csv`

### **Combined Data Columns**

#### **Original Phase 1 Columns (preserved):**
- `has_know_more_link`, `phase2_ready`, `state`, `district`
- `school_name`, `know_more_link`, `edu_district`, `edu_block`
- `academic_year`, `school_category`, `school_management`
- `class_range`, `school_type`, `school_location`, `address`, `pin_code`

#### **New Phase 2 Columns (extracted):**
- `detail_school_name` - School name from detail page
- `source_url` - URL visited for extraction
- `extraction_timestamp` - When data was extracted
- `location` - Detailed location information
- `class_from`, `class_to` - Class range details
- `year_of_establishment` - When school was established
- `national_management`, `state_management` - Management details
- `affiliation_board_sec`, `affiliation_board_hsec` - Board affiliations
- `total_students`, `total_boys`, `total_girls` - Student enrollment
- `total_teachers`, `male_teachers`, `female_teachers` - Teacher data
- `extraction_status` - SUCCESS/PARTIAL/FAILED
- `fields_extracted` - Number of fields successfully extracted
- `critical_fields_extracted` - Number of critical fields extracted

## Usage Instructions

### **Step 1: Configure Input File**
1. Place your Phase 1 CSV file in the same directory as the script
2. Edit `INPUT_CSV_FILE` variable in the script to point to your file
3. Optionally set `MAX_RECORDS_TO_PROCESS` for testing

### **Step 2: Run the Script**
```bash
python standalone_phase2_processor.py
```

### **Step 3: Monitor Progress**
The script provides real-time updates:
- Current school being processed
- Extraction success/failure for each school
- Progress percentage and time estimates
- Final summary with success rates

### **Example Output:**
```
üöÄ STANDALONE PHASE 2 PROCESSOR
üìÅ Input file: GOA_phase1_complete_20250809_103414.csv

üè´ Processing school 1/1594: AASTHA ANAND NIKETAN SCHOOL
   üìã UDISE Code: 4800722
‚úÖ EXCELLENT extraction
   üéØ Critical fields: 2/2, Total fields: 7
   üë• Students: 70, Teachers: 9
   ‚úÖ Successfully processed and saved

üìä PROGRESS UPDATE:
   üéØ Processed: 10/1594 (0.6%)
   ‚úÖ Successful: 9
   ‚ùå Failed: 1
   ‚è±Ô∏è Elapsed: 2.1 minutes
   ‚è±Ô∏è Estimated remaining: 332.4 minutes
```

## Crash Recovery

### **Automatic Resume**
If the script is interrupted, it can automatically resume:
1. The script checks the output CSV file for already processed schools
2. Skips schools that have already been processed (based on UDISE code)
3. Continues from where it left off

### **Manual Recovery**
If needed, you can manually resume by:
1. Keeping the existing output CSV file
2. Running the script again with the same configuration
3. The script will automatically detect and skip processed records

## Performance Characteristics

### **Processing Speed**
- **Average**: 10-15 seconds per school
- **Factors**: Network speed, page complexity, extraction success
- **Optimization**: Browser settings optimized for speed

### **Resource Usage**
- **Memory**: Moderate (Chrome browser + data processing)
- **Network**: High (downloading school detail pages)
- **Storage**: Low (incremental CSV writing)

### **Scalability**
- **Small datasets** (< 100 schools): 15-30 minutes
- **Medium datasets** (100-1000 schools): 2-5 hours
- **Large datasets** (1000+ schools): 5-15 hours

## Error Handling

### **Network Issues**
- Automatic retry for failed page loads
- Timeout handling for slow responses
- Graceful degradation for connection problems

### **Extraction Failures**
- Partial data extraction when possible
- Failure markers in output data
- Detailed logging for debugging

### **Browser Issues**
- Automatic browser restart on crashes
- Optimized browser settings for stability
- Memory management for long-running processes

## Troubleshooting

### **Common Issues**

#### **"Input file not found"**
- Check the `INPUT_CSV_FILE` path
- Ensure the file exists in the correct directory
- Use absolute path if needed

#### **"No schools with valid know_more_link URLs"**
- Verify the CSV has a `know_more_link` column
- Check that URLs are valid and not empty
- Ensure URLs contain "http" protocol

#### **Browser crashes**
- Close other Chrome instances
- Restart the script
- Check available memory

### **Performance Issues**
- Reduce `MAX_RECORDS_TO_PROCESS` for testing
- Close unnecessary applications
- Check network connectivity

## Best Practices

### **Before Running**
1. **Test with small dataset** - Set `MAX_RECORDS_TO_PROCESS = 10`
2. **Verify input file** - Check CSV structure and URLs
3. **Close other browsers** - Avoid resource conflicts
4. **Ensure stable network** - Reliable internet connection

### **During Processing**
1. **Monitor progress** - Check logs for issues
2. **Don't interrupt** - Let it complete or use Ctrl+C gracefully
3. **Check output file** - Verify data quality periodically

### **After Completion**
1. **Verify output** - Check final CSV file
2. **Review success rate** - Investigate failures if needed
3. **Backup results** - Save output files safely

## Integration with Existing Workflow

### **Phase 1 ‚Üí Standalone Phase 2**
1. Run Phase 1 scraping to generate CSV files
2. Use `standalone_phase2_processor.py` to process each state file
3. Get combined Phase 1 + Phase 2 data in single CSV

### **Batch Processing Multiple States**
```bash
# Process multiple state files
python standalone_phase2_processor.py  # Configure for STATE1
python standalone_phase2_processor.py  # Configure for STATE2
# etc.
```

### **Google Sheets Integration**
The output CSV files can be uploaded to Google Sheets using existing upload scripts or manually imported.

## Success Metrics

### **Excellent Results** (Target)
- **Success Rate**: > 90%
- **Critical Fields**: 2/2 (Students + Teachers)
- **Total Fields**: > 5 per school

### **Acceptable Results**
- **Success Rate**: > 70%
- **Critical Fields**: 1/2 (Either Students or Teachers)
- **Total Fields**: > 3 per school

### **Review Required**
- **Success Rate**: < 70%
- **Critical Fields**: 0/2 (Neither Students nor Teachers)
- **Total Fields**: < 3 per school

## Conclusion

The Standalone Phase 2 Processor provides a robust, independent solution for extracting detailed school data from Phase 1 CSV files. With comprehensive error handling, crash recovery, and incremental writing, it's designed for production use with large datasets while maintaining data integrity and processing reliability.
